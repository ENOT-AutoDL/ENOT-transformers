{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83e43b0-4a77-4e85-8ff4-59c0f6a70126",
   "metadata": {},
   "source": [
    "## Install all requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba79bed4-930e-4f75-bdd9-64c530695136",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815d766e-7160-4095-9d02-7be2f85580de",
   "metadata": {},
   "source": [
    "## Download onnx and build engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2673a8cc-66e3-4b59-b06a-7d90921a9441",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9fd130-1e2d-45c7-b99f-1a10f33988cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "from utils.download import download_model_onnx\n",
    "from utils.trt_builder import DefaultTransformerEngineBuilder\n",
    "\n",
    "\n",
    "def get_engine_path(\n",
    "    model_name: str,\n",
    "    repo_id: str,\n",
    "    max_batch_size: int = 1,\n",
    "    max_seq_len: int = 256,\n",
    "    max_history_len: int = 512,\n",
    "    force_rebuild: bool = False,\n",
    "    cache_dir: Union[str, Path] = 'cache',\n",
    ") -> Path:\n",
    "    engine_cache_path = Path(cache_dir) / f'{model_name}-b{max_batch_size}s{max_seq_len}h{max_history_len}.engine'\n",
    "    if not force_rebuild and engine_cache_path.is_file() and engine_cache_path.exists():\n",
    "        return engine_cache_path\n",
    "\n",
    "    path_to_onnx = download_model_onnx(model_name=model_name, repo_id=repo_id, cache_dir=cache_dir)\n",
    "    builder = DefaultTransformerEngineBuilder(\n",
    "        max_batch_size=max_batch_size,\n",
    "        max_seq_len=max_seq_len,\n",
    "        max_history_len=max_history_len,\n",
    "        use_fp16=False,\n",
    "        use_int8=True,\n",
    "    )\n",
    "    builder.build(path_to_onnx=path_to_onnx, engine_cache_path=engine_cache_path)\n",
    "    return engine_cache_path\n",
    "\n",
    "\n",
    "engine_path = get_engine_path(model_name='gpt2-xl-i8', repo_id='ENOT-AutoDL/gpt2-tensorrt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d33cfd-15a2-4184-860d-549f72887655",
   "metadata": {},
   "source": [
    "## Initialize and test seq2seq model.\n",
    "### All important information with TensorRT initialization you can find in `utils/trt_model.py` and `utils/trt_seq2seq_model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca46628f-a940-4be0-9904-a5ee0ca161a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from utils.trt_seq2seq_model import TrtSeq2SeqModel\n",
    "\n",
    "\n",
    "model = TrtSeq2SeqModel(path_to_engine=engine_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "input_text = 'Hello world!'\n",
    "\n",
    "input_ids = tokenizer(input_text, return_tensors='pt')\n",
    "input_ids = input_ids['input_ids'].to(device='cuda', dtype=torch.int32)\n",
    "generated_ids = model.generate(input_ids, generate_len=100)\n",
    "(generated_ids,) = generated_ids\n",
    "generated_text = tokenizer.decode(generated_ids)\n",
    "\n",
    "print('=' * 100)\n",
    "print(input_text + generated_text)\n",
    "print('=' * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9e1ed1-b2b9-4f52-aa03-a1467fc396da",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Accuracy validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a36706-4e3b-4c39-8e7b-fced32384cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.test import test_acc\n",
    "\n",
    "\n",
    "test_acc(\n",
    "    lambda input_ids: model.generate(input_ids, generate_len=1),\n",
    "    device='cuda',\n",
    "    verbose=True,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6349e6-8d52-4cad-b282-52dd91c74dc3",
   "metadata": {},
   "source": [
    "## Latency test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a79e243-3be0-4815-a2ba-3a0b321f1aa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "from utils.test import test_latency\n",
    "\n",
    "\n",
    "def generate_ids_function(seq_len: int) -> torch.Tensor:\n",
    "    return torch.ones(size=(1, seq_len), device='cuda', dtype=torch.int32)\n",
    "\n",
    "\n",
    "def generate_seq_function(input_ids: torch.Tensor, generate_len: int) -> torch.Tensor:\n",
    "    return model.generate(input_ids, generate_len=generate_len)\n",
    "\n",
    "\n",
    "test_latency(\n",
    "    generate_ids_function=generate_ids_function,\n",
    "    generate_seq_function=generate_seq_function,\n",
    "    variants=list(product([64, 128, 256], [64, 128, 256])),\n",
    "    warmup=20,\n",
    "    repeats=20,\n",
    "    verbose=True,\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
